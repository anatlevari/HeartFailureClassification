{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae84532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /var/folders/73/0j93mrvd1s77hv6p1fbfrqz80000gn/T/matplotlib-1xg25zly because the default path (/Users/anatlevari/.matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# classifiers\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# to compare and evaluate our classifiers\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#ignore warnings for now\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12071da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/anatlevari/Downloads/heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45252f0",
   "metadata": {},
   "source": [
    "# Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b069d",
   "metadata": {},
   "source": [
    "In this notebook, different classification algorithms will be implemented on the Heart Failure dataset. Their accuracy is compared, before and after cross-validation (KFold) is implemented. In addition, for KNN algorithm, the optimal value of k is obtained. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41250526",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "For some of the algorithms, transforming the categorical features into numerical ones is essential. This replacement is implemented below, with the separation of the data into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93876601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the different values of the catagorical features\n",
    "df['Sex'].unique()\n",
    "df['ChestPainType'].unique()\n",
    "df['RestingECG'].unique()\n",
    "df['ExerciseAngina'].unique()\n",
    "df['ST_Slope'].unique()\n",
    "\n",
    "# replace non numerical features with numerical ones\n",
    "df = df.replace(['F', 'M'], [0,1])\n",
    "df = df.replace(['ATA', 'NAP', 'ASY', 'TA'], [0,1,2,3])\n",
    "df = df.replace(['Normal', 'ST', 'LVH'], [0,1,2])\n",
    "df = df.replace(['N', 'Y'], [0,1])\n",
    "df = df.replace(['Up', 'Flat', 'Down'], [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4591e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate input (X) to output (y)\n",
    "X = df.iloc[:,:-1].values  #input\n",
    "y = df.iloc[:, 11].values  # output - target\n",
    "\n",
    "# divide the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
    "\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551bfe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross-validation - KFold\n",
    "#cv = KFold(n_splits=25, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579d161",
   "metadata": {},
   "source": [
    "## SVM Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5ecdd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of SVM classifier on training set: 90.46\n",
      "Accuracy of SVM classifier on test set: 85.33\n",
      "Accuracy of SVM classifier on test set, using 10Fold cv: 70.89\n",
      "Standard Deviation of the cv accuracy score: 8.26\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# accuracy of svm\n",
    "svm_acc = round(svm.score(X_test,y_test)*100, 2)\n",
    "\n",
    "# accuracy using cv\n",
    "smv_cv_scores = cross_val_score(svm, X, y, scoring='accuracy', cv=10)\n",
    "svm_acc_cv = round(smv_cv_scores.mean() * 100, 2)\n",
    "\n",
    "print('Accuracy of SVM classifier on training set: {:.2f}'\n",
    "     .format(round(svm.score(X_train, y_train)*100, 2)))\n",
    "print('Accuracy of SVM classifier on test set: {:.2f}'\n",
    "     .format(svm_acc))\n",
    "print('Accuracy of SVM classifier on test set, using 10Fold cv: {:.2f}'\n",
    "     .format(svm_acc_cv))\n",
    "\n",
    "# Verify balance of the data in the division of cv  \n",
    "print(\"Standard Deviation of the cv accuracy score:\", round(smv_cv_scores.std()*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21523454",
   "metadata": {},
   "source": [
    "As seen above, the accuracy after applying cv decrease. The reason may be revealed after looking into the accuracy of each fold more closely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dd6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [0.6521739130434783, 0.717391304347826, 0.6630434782608695, 0.6630434782608695, 0.7391304347826086, 0.7391304347826086, 0.6630434782608695, 0.782608695652174, 0.7692307692307693, 0.7802197802197802]\n",
      "Avg accuracy : 0.7169015766841853\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "kf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
    "model = SVC()\n",
    " \n",
    "acc_score = []\n",
    " \n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train , X_test = X[train_index,:],X[test_index,:]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = accuracy_score(pred_values , y_test)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = sum(acc_score)/k\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f797d4",
   "metadata": {},
   "source": [
    "We can see major difference in the accuracy of the different folds, even after applying \"shuffle\" option. This can be caused by uneven distribution of some of the features (not the target one). This was seen in the first notebook (\"Heart Failure - Insights\"). \n",
    "Thus, for this classifier, we will consider the accuracy without cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963cc22",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef628c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 85.37\n",
      "Accuracy of Logistic Regression classifier on test set: 84.62\n",
      "Accuracy of Logistic Regression classifier on test set, using 120Fold cv: 84.67\n",
      "Standard Deviation of the cv accuracy score: 13.17\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# accuracy of logreg\n",
    "logreg_acc = round(logreg.score(X_test,y_test) * 100, 2)\n",
    "\n",
    "# accurecy using cv\n",
    "logreg_scores = cross_val_score(logreg, X, y, scoring='accuracy', cv=120)\n",
    "logreg_acc_cv = round(logreg_scores.mean() * 100, 2)\n",
    "\n",
    "\n",
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(round(logreg.score(X_train, y_train)*100, 2)))\n",
    "print('Accuracy of Logistic Regression classifier on test set: {:.2f}'\n",
    "     .format(logreg_acc))\n",
    "print('Accuracy of Logistic Regression classifier on test set, using 120Fold cv: {:.2f}'\n",
    "     .format(logreg_acc_cv))\n",
    "\n",
    "# Verify balance of the data in the division of cv  \n",
    "print(\"Standard Deviation of the cv accuracy score:\", round(logreg_scores.std()*100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baba1505",
   "metadata": {},
   "source": [
    "## KNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56134007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNN classifier on training set: 79.08\n",
      "Accuracy of KNN classifier on test set: 73.63\n",
      "Accuracy of KNN on test set, using 50Fold cv: 70.94\n",
      "Standard Deviation of the cv accuracy score: 12.89\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# accuracy of knn\n",
    "knn_acc = round(knn.score(X_test,y_test) * 100, 2)\n",
    "\n",
    "# accuracy of knn using cv\n",
    "knn_scores = cross_val_score(knn, X, y, scoring='accuracy', cv=50)\n",
    "knn_acc_cv = round(knn_scores.mean() * 100, 2)\n",
    "\n",
    "\n",
    "print('Accuracy of KNN classifier on training set: {:.2f}'\n",
    "     .format(round(knn.score(X_train, y_train)*100, 2)))\n",
    "print('Accuracy of KNN classifier on test set: {:.2f}'\n",
    "     .format(knn_acc))\n",
    "print('Accuracy of KNN on test set, using 50Fold cv: {:.2f}'\n",
    "     .format(knn_acc_cv))\n",
    "\n",
    "# Verify balance of the data in the division of cv  \n",
    "print(\"Standard Deviation of the cv accuracy score:\", round(knn_scores.std()*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d9aa6",
   "metadata": {},
   "source": [
    "As before, an examination of the accuracy on the different folds is required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074e5315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of each fold - [57.89, 94.74, 63.16, 63.16, 73.68, 52.63, 68.42, 68.42, 73.68, 68.42, 68.42, 89.47, 78.95, 84.21, 63.16, 68.42, 73.68, 73.68, 72.22, 55.56, 77.78, 72.22, 55.56, 77.78, 66.67, 66.67, 61.11, 66.67, 66.67, 55.56, 61.11, 72.22, 55.56, 50.0, 77.78, 83.33, 66.67, 88.89, 77.78, 66.67, 83.33, 61.11, 66.67, 61.11, 66.67, 77.78, 61.11, 66.67, 83.33, 83.33]\n",
      "Avg accuracy : 69.8\n"
     ]
    }
   ],
   "source": [
    "k = 50\n",
    "kf = KFold(n_splits=k, random_state=None, shuffle=True)\n",
    "model = KNeighborsClassifier()\n",
    " \n",
    "acc_score = []\n",
    " \n",
    "for train_index , test_index in kf.split(X):\n",
    "    X_train , X_test = X[train_index,:],X[test_index,:]\n",
    "    y_train , y_test = y[train_index] , y[test_index]\n",
    "     \n",
    "    model.fit(X_train,y_train)\n",
    "    pred_values = model.predict(X_test)\n",
    "     \n",
    "    acc = round(accuracy_score(pred_values , y_test)*100,2)\n",
    "    acc_score.append(acc)\n",
    "     \n",
    "avg_acc_score = round(sum(acc_score)/k,2)\n",
    " \n",
    "print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c4811",
   "metadata": {},
   "source": [
    "As we saw with the SVM classifier, here the distribution of some of the features is uneven over the different folds. Thus, we will consider the accuracy without cv.\n",
    "In addition, we shall find the optimal k (=number of neighbors to compare), i.e., the number k which maximize the accuracy, and save it as the accuracy of the knn classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c396aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of neighbors in the kNN algorithm is: {'n_neighbors': 39}\n",
      "The accuracy is: 69.0\n"
     ]
    }
   ],
   "source": [
    "# check to find the best k for knn model\n",
    "\n",
    "#create new a knn model\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, X_train.shape[0])}\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gscv = GridSearchCV(knn2, param_grid)\n",
    "\n",
    "#fit model to data\n",
    "knn_gscv.fit(X_train, y_train)\n",
    "\n",
    "#check mean score for the top performing value of n_neighbors\n",
    "knn_gscv.best_score_\n",
    "\n",
    "#check top performing n_neighbors value\n",
    "knn_gscv.best_params_\n",
    "\n",
    "print('The optimal number of neighbors in the kNN algorithm is:', knn_gscv.best_params_)\n",
    "print('The accuracy is:', round(knn_gscv.best_score_*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10e215a",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b40cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Naive Bayes classifier on training set: 84.67\n",
      "Accuracy of Naive Bayes classifier on test set: 88.89\n",
      "Accuracy of Naive Bayes on test set, using 100Fold cv: 84.69\n",
      "Standard Deviation of the cv accuracy score: 0.13200804315188575\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# naive bayes accuracy\n",
    "gnb_acc = round(gnb.score(X_test,y_test) * 100, 2)\n",
    "\n",
    "# naive bayes accuracy using cv\n",
    "gnb_scores = cross_val_score(gnb, X, y, scoring='accuracy', cv=100)\n",
    "gnb_acc_cv = round(gnb_scores.mean() * 100, 2)\n",
    "\n",
    "\n",
    "print('Accuracy of Naive Bayes classifier on training set: {:.2f}'\n",
    "     .format(round(gnb.score(X_train, y_train)*100, 2)))\n",
    "print('Accuracy of Naive Bayes classifier on test set: {:.2f}'\n",
    "     .format(gnb_acc))\n",
    "print('Accuracy of Naive Bayes on test set, using 100Fold cv: {:.2f}'\n",
    "     .format(gnb_acc_cv))\n",
    "\n",
    "# Verify balance of the data in the division of cv  \n",
    "print(\"Standard Deviation of the cv accuracy score:\", gnb_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7500c75a",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d71e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 100.00\n",
      "Accuracy of Decision Tree classifier on test set: 88.89\n",
      "Accuracy of Decision Tree classifier on test set, using 80Fold cv: 77.20\n",
      "Standard Deviation of the cv accuracy score: 0.11923457869534945\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# decision tree accuracy\n",
    "clf_acc = round(clf.score(X_test,y_test) * 100, 2)\n",
    "\n",
    "# decision tree accuracy using cv\n",
    "clf_scores = cross_val_score(clf, X, y, scoring='accuracy', cv=80)\n",
    "clf_acc_cv = round(clf_scores.mean() * 100, 2)\n",
    "\n",
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(round(clf.score(X_train, y_train)*100, 2)))\n",
    "print('Accuracy of Decision Tree classifier on test set: {:.2f}'\n",
    "     .format(clf_acc))\n",
    "print('Accuracy of Decision Tree classifier on test set, using 80Fold cv: {:.2f}'\n",
    "     .format(clf_acc_cv))\n",
    "\n",
    "# Verify balance of the data in the division of cv  \n",
    "print(\"Standard Deviation of the cv accuracy score:\", clf_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10316099",
   "metadata": {},
   "source": [
    "### Note: \n",
    "The difference between the accuracy over the train and dataset implies overfitting. This can be seen also in the next classifier - Random Forest. The overfitting and other metrics of evaluation of the decision tree classifier will be examined further in a new notebook - 'Heart Failure - Decision Tree in Details'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056b9e7",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d2cdbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest classifier on training set: 100.00\n",
      "Accuracy of Random Forest classifier on test set: 94.44\n",
      "Accuracy of Random Forest classifier on test set, using 80Fold cv: 86.68\n",
      "Standard Deviation of the cv accuracy score: 0.11365968037300259\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=250)\n",
    "rf.fit(X_train,y_train)\n",
    "\n",
    "# random forest accuracy\n",
    "rf_acc = round(rf.score(X_test,y_test) * 100, 2)\n",
    "\n",
    "# random forest accuracy using cv\n",
    "rf_scores = cross_val_score(rf, X, y, scoring='accuracy', cv=80)\n",
    "rf_acc_cv = round(rf_scores.mean() * 100, 2)\n",
    "\n",
    "print('Accuracy of Random Forest classifier on training set: {:.2f}'\n",
    "     .format(round(rf.score(X_train, y_train)*100, 2)))\n",
    "print('Accuracy of Random Forest classifier on test set: {:.2f}'\n",
    "     .format(rf_acc))\n",
    "print('Accuracy of Random Forest classifier on test set, using 80Fold cv: {:.2f}'\n",
    "     .format(rf_acc_cv))\n",
    "\n",
    "# Verify balance of the data in the division of cv  \n",
    "print(\"Standard Deviation of the cv accuracy score:\", rf_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d73baed",
   "metadata": {},
   "source": [
    "# Algorithm Comparison\n",
    "Compare the accuracy score on the testset, with cv and without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0061d762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "      <th>Score after Cross-Validation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>94.44</td>\n",
       "      <td>86.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>88.89</td>\n",
       "      <td>84.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>88.89</td>\n",
       "      <td>77.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>85.33</td>\n",
       "      <td>70.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>84.62</td>\n",
       "      <td>84.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>73.63</td>\n",
       "      <td>70.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Score  Score after Cross-Validation\n",
       "5            Random Forest  94.44                         86.68\n",
       "3              Naive Bayes  88.89                         84.69\n",
       "4            Decision Tree  88.89                         77.20\n",
       "0  Support Vector Machines  85.33                         70.89\n",
       "1      Logistic Regression  84.62                         84.67\n",
       "2                      KNN  73.63                         70.94"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'Logistic Regression', 'KNN', 'Naive Bayes', 'Decision Tree',\n",
    "              'Random Forest'],\n",
    "    'Score': [svm_acc, logreg_acc, knn_acc, gnb_acc, clf_acc, rf_acc],\n",
    "'Score after Cross-Validation': [svm_acc_cv, logreg_acc_cv, knn_acc_cv, gnb_acc_cv, clf_acc_cv, rf_acc_cv]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953750f0",
   "metadata": {},
   "source": [
    "# Conclusion and Future Work:\n",
    "1. The maximal accuracy was achieved by a random forest classifier and is equal to 94.44%. Thus, this classifier may be a good fit for future heart failure detection.\n",
    "2. Cross-validation technique did not improve the accuracy. It revealed the bias found in some of the features. \n",
    "3. Naive Bayes classifier must be re-checked for this purpose since it requires uncorrelated features. As seen in the heat map from the previous notebook, there is some (small) correlation between a few features of the model. I've added this classifier here for practice reasons.\n",
    "4. The overfitting in the Decision tree and Random Forest classifiers will be addressed separately in the next notebook. \n",
    "5. Next notebook will also use the interpretability of the decision tree classifier to clarify the results and determine the features leading to a higher risk of heart failure. \n",
    "6. It will also address other metrics to evaluate the classification models, beyond accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a540a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
